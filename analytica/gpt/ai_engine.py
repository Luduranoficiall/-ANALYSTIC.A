# ============================================
# ANALYSTIC.A ‚Äî AI ENGINE (OLLAMA + GEMINI)
# 100% GRATUITO!
# ============================================
import os
import json
import httpx
from typing import Optional

# ============================================
# CONFIGURA√á√ïES
# ============================================
OLLAMA_URL = "http://localhost:11434"
OLLAMA_MODEL = "gemma:2b"  # Modelo local instalado

# Gemini 1.5 Flash API (gratuito com limite generoso)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"


# ============================================
# OLLAMA (LOCAL - 100% GR√ÅTIS)
# ============================================
async def ollama_generate(prompt: str, model: str = OLLAMA_MODEL) -> Optional[str]:
    """
    Gera resposta usando Ollama local.
    Totalmente offline e gratuito!
    """
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{OLLAMA_URL}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_predict": 1024
                    }
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get("response", "")
            else:
                print(f"Ollama error: {response.status_code}")
                return None
                
    except Exception as e:
        print(f"Ollama connection error: {e}")
        return None


def ollama_generate_sync(prompt: str, model: str = OLLAMA_MODEL) -> Optional[str]:
    """Vers√£o s√≠ncrona do Ollama"""
    try:
        import requests
        response = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 1024
                }
            },
            timeout=60
        )
        
        if response.status_code == 200:
            return response.json().get("response", "")
        return None
        
    except Exception as e:
        print(f"Ollama sync error: {e}")
        return None


# ============================================
# GEMINI (GOOGLE - GRATUITO COM LIMITES)
# ============================================
async def gemini_generate(prompt: str) -> Optional[str]:
    """
    Gera resposta usando Google Gemini.
    Gratuito: 60 requisi√ß√µes/minuto
    """
    if not GEMINI_API_KEY:
        print("‚ö†Ô∏è GEMINI_API_KEY n√£o configurada")
        return None
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{GEMINI_URL}?key={GEMINI_API_KEY}",
                json={
                    "contents": [{
                        "parts": [{
                            "text": prompt
                        }]
                    }],
                    "generationConfig": {
                        "temperature": 0.7,
                        "topP": 0.9,
                        "maxOutputTokens": 1024
                    }
                },
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                data = response.json()
                candidates = data.get("candidates", [])
                if candidates:
                    content = candidates[0].get("content", {})
                    parts = content.get("parts", [])
                    if parts:
                        return parts[0].get("text", "")
            else:
                print(f"Gemini error: {response.status_code} - {response.text}")
                return None
                
    except Exception as e:
        print(f"Gemini error: {e}")
        return None


def gemini_generate_sync(prompt: str) -> Optional[str]:
    """Vers√£o s√≠ncrona do Gemini"""
    if not GEMINI_API_KEY:
        return None
        
    try:
        import requests
        response = requests.post(
            f"{GEMINI_URL}?key={GEMINI_API_KEY}",
            json={
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 1024
                }
            },
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            candidates = data.get("candidates", [])
            if candidates:
                return candidates[0]["content"]["parts"][0]["text"]
        return None
        
    except Exception as e:
        print(f"Gemini sync error: {e}")
        return None


# ============================================
# FUN√á√ÉO PRINCIPAL - TENTA OLLAMA, DEPOIS GEMINI
# ============================================
async def generate_ai_response(prompt: str, prefer_local: bool = True) -> dict:
    """
    Gera resposta usando IA dispon√≠vel.
    Prioriza Ollama (local/gratuito), fallback para Gemini.
    
    Returns:
        dict: {"response": str, "source": str, "success": bool}
    """
    
    # Prompt otimizado para an√°lise de dados
    system_prompt = """Voc√™ √© o ANALYSTIC.IA, um assistente especializado em an√°lise de dados.
Responda de forma clara, objetiva e em portugu√™s brasileiro.
Use emojis para deixar as respostas mais visuais.
Foque em insights acion√°veis e dados relevantes."""

    full_prompt = f"{system_prompt}\n\nUsu√°rio: {prompt}\n\nAssistente:"
    
    if prefer_local:
        # Tenta Ollama primeiro (local, gr√°tis, sem limites)
        response = await ollama_generate(full_prompt)
        if response:
            return {
                "response": response,
                "source": "ü¶ô Ollama (Local)",
                "model": OLLAMA_MODEL,
                "success": True
            }
        
        # Fallback para Gemini
        response = await gemini_generate(full_prompt)
        if response:
            return {
                "response": response,
                "source": "‚ú® Google Gemini",
                "model": "gemini-pro",
                "success": True
            }
    else:
        # Tenta Gemini primeiro
        response = await gemini_generate(full_prompt)
        if response:
            return {
                "response": response,
                "source": "‚ú® Google Gemini",
                "model": "gemini-pro",
                "success": True
            }
        
        # Fallback para Ollama
        response = await ollama_generate(full_prompt)
        if response:
            return {
                "response": response,
                "source": "ü¶ô Ollama (Local)",
                "model": OLLAMA_MODEL,
                "success": True
            }
    
    return {
        "response": "‚ùå Nenhuma IA dispon√≠vel no momento. Verifique se o Ollama est√° rodando (ollama serve) ou configure a GEMINI_API_KEY.",
        "source": "Sistema",
        "model": None,
        "success": False
    }


def generate_insights(question: str) -> str:
    """
    Fun√ß√£o compat√≠vel com o sistema existente.
    Usa vers√£o s√≠ncrona.
    """
    # Tenta Ollama primeiro
    response = ollama_generate_sync(question)
    if response:
        return f"{response}\n\nü¶ô _Gerado por Ollama ({OLLAMA_MODEL})_"
    
    # Tenta Gemini
    response = gemini_generate_sync(question)
    if response:
        return f"{response}\n\n‚ú® _Gerado por Google Gemini_"
    
    return "‚ùå IA n√£o dispon√≠vel. Execute: `ollama serve` para iniciar o Ollama."


# ============================================
# FUN√á√ïES ESPEC√çFICAS PARA AN√ÅLISE DE DADOS
# ============================================
async def analyze_data(data_description: str) -> dict:
    """Analisa dados e retorna insights"""
    prompt = f"""Analise os seguintes dados e forne√ßa:
1. üìä Resumo estat√≠stico
2. üìà Tend√™ncias identificadas
3. üí° 3 insights principais
4. ‚ö†Ô∏è Pontos de aten√ß√£o
5. üéØ Recomenda√ß√µes

Dados:
{data_description}"""
    
    return await generate_ai_response(prompt)


async def predict_trend(historical_data: str) -> dict:
    """Faz previs√£o baseada em dados hist√≥ricos"""
    prompt = f"""Com base nos dados hist√≥ricos abaixo, fa√ßa uma previs√£o para os pr√≥ximos 3 meses:

{historical_data}

Forne√ßa:
1. üîÆ Previs√£o de valores
2. üìä Tend√™ncia esperada (crescimento/queda/est√°vel)
3. üéØ N√≠vel de confian√ßa
4. ‚ö†Ô∏è Fatores de risco"""
    
    return await generate_ai_response(prompt)


async def explain_chart(chart_description: str) -> dict:
    """Explica um gr√°fico para o usu√°rio"""
    prompt = f"""Explique o seguinte gr√°fico de forma clara e did√°tica:

{chart_description}

Inclua:
1. üìù O que o gr√°fico mostra
2. üìà Principais tend√™ncias
3. üí° Insights importantes
4. üéØ Conclus√µes pr√°ticas"""
    
    return await generate_ai_response(prompt)


async def suggest_visualization(data_type: str) -> dict:
    """Sugere o melhor tipo de visualiza√ß√£o para os dados"""
    prompt = f"""Para os seguintes dados: {data_type}

Sugira:
1. üìä Melhor tipo de gr√°fico
2. üé® Cores recomendadas
3. üìê Layout ideal
4. üí° Dicas de visualiza√ß√£o"""
    
    return await generate_ai_response(prompt)


# ============================================
# CHAT CONVERSACIONAL
# ============================================
class AIChat:
    def __init__(self):
        self.history = []
        self.max_history = 10
    
    async def send_message(self, message: str) -> dict:
        """Envia mensagem mantendo contexto da conversa"""
        
        # Monta contexto com hist√≥rico
        context = "\n".join([
            f"{'Usu√°rio' if i % 2 == 0 else 'IA'}: {msg}"
            for i, msg in enumerate(self.history[-6:])  # √öltimas 3 trocas
        ])
        
        full_prompt = f"""Contexto da conversa:
{context}

Usu√°rio: {message}

Responda de forma √∫til e contextualizada:"""
        
        response = await generate_ai_response(full_prompt)
        
        # Salva no hist√≥rico
        self.history.append(message)
        if response["success"]:
            self.history.append(response["response"])
        
        # Limita hist√≥rico
        if len(self.history) > self.max_history * 2:
            self.history = self.history[-self.max_history * 2:]
        
        return response
    
    def clear_history(self):
        """Limpa hist√≥rico da conversa"""
        self.history = []


# Inst√¢ncia global do chat
ai_chat = AIChat()
